{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2c8f5e-7c07-456a-9f58-4c212039c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import openpyxl\n",
    "import os\n",
    "import json\n",
    "import geopandas as gpd\n",
    "\n",
    "pd.set_option('display.max_rows',1000)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.width',1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Note: only load the shapefile dataframe when needed to merge. This will inflate the filesize of the generated csv\n",
    "\n",
    "# shp_path = os.path.join('..', 'philippines-psgc-shapefiles', 'philippines-psgc-shapefiles', 'data', '2023', 'Municities', 'phl_admbnda_adm3_psa_namria_20231106.shp')\n",
    "# shape_file_df = gpd.read_file(shp_path)\n",
    "# map_df = shape_file_df[['ADM3_EN', 'ADM2_EN', 'ADM1_EN', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5964a0-de57-4a20-9864-98687d94855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Constants\n",
    "\n",
    "SAVE_FILE_TYPE = 'csv'\n",
    "YEARS = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca2dec-069a-416b-a351-ee5ed455d848",
   "metadata": {},
   "source": [
    "# Processing Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746030a6-4838-42a8-98ec-329d9ff77daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Summary Statistics: 100%|███████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "CPU times: total: 1.33 s\n",
      "Wall time: 1.43 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def preprocess_port_data(file_path):\n",
    "\n",
    "    data = pd.read_excel(file_path, header=[0, 1, 2, 3])\n",
    "    \n",
    "    new_columns = []\n",
    "    for col in data.columns:\n",
    "        col_names = [str(c).strip() for c in col if 'Unnamed' not in str(c)]\n",
    "        new_col_name = ' | '.join(col_names) if col_names else 'Unknown'\n",
    "        new_columns.append(new_col_name)\n",
    "    \n",
    "    data.columns = new_columns\n",
    "    data.dropna(how='all', axis=1, inplace=True)\n",
    "    data = data.loc[~(data.map(lambda x: pd.isna(x) or str(x).strip() == '')).all(axis=1)]\n",
    "    \n",
    "    data.dropna(how='all', axis=0, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Hardcode renaming the first three columns to 'Category 1', 'Category 2', 'Category 3'\n",
    "    data.columns.values[0] = 'Category 1'\n",
    "    data.columns.values[1] = 'Category 2'\n",
    "    data.columns.values[2] = 'Category 3'\n",
    "\n",
    "    # Add the quarter to the 'TOTAL' column names correctly\n",
    "    quarters = ['1st Quarter', '2nd Quarter', '3rd Quarter', '4th Quarter']\n",
    "    for quarter in quarters:\n",
    "        for i, col in enumerate(data.columns):\n",
    "            if col == 'TOTAL' and i > 0 and quarter in data.columns[i+1]:\n",
    "                data.columns.values[i] = f'{quarter} TOTAL'\n",
    "\n",
    "    # Fix 'GRAND TOTAL' columns\n",
    "    data.columns = [col.replace('GRAND TOTAL | ', 'GRAND TOTAL | ') for col in data.columns]\n",
    "    data.columns = [col.replace('Unknown', '') for col in data.columns]\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data['Category 1'].ffill(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "for year in tqdm(YEARS, desc='Processing Summary Statistics'):\n",
    "    file_path = os.path.join('..', '[DATA] Ports Files', 'Raw Files', year, f'{year}SummaryStatistics.xlsx')\n",
    "    save_path = os.path.join('..', '[DATA] Ports Files', 'Standardized', 'Summary Statistics', f'{year}SummaryStatistics.{SAVE_FILE_TYPE}')\n",
    "    cleaned_data = preprocess_port_data(file_path)\n",
    "    cleaned_data.to_csv(save_path, index=False)\n",
    "\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab316f-b168-412e-8626-a607f20fe4e2",
   "metadata": {},
   "source": [
    "# Processing Shipping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92b497c-7b92-4289-9d0f-6fbdb83a856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Shipping data:   0%|                                                                  | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '..\\\\[DATA] Ports Files\\\\Standardized\\\\Shipping\\\\2023 - Shipping.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:135\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3903\u001b[0m     path_or_buf,\n\u001b[0;32m   3904\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3905\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3906\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3907\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3908\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3909\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3910\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3911\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3912\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3913\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3914\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3915\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3916\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3917\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3918\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3919\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    251\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    252\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    253\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    254\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '..\\\\[DATA] Ports Files\\\\Standardized\\\\Shipping\\\\2023 - Shipping.csv'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def map_city(pmo, pier_terminal):\n",
    "    \"\"\"Map the city based on PMO and Pier/Terminal using the provided mapping.\"\"\"\n",
    "    mapping_file = os.path.join('..', 'port_city_mapper.json')\n",
    "    with open(mapping_file, 'r') as f:\n",
    "        port_city_mapper = json.load(f)\n",
    "    \n",
    "    if pd.isna(pmo) or pd.isna(pier_terminal):\n",
    "        return pier_terminal\n",
    "    \n",
    "    pmo = pmo.upper()\n",
    "    pier_terminal = pier_terminal.upper()\n",
    "    \n",
    "    if pmo in port_city_mapper:\n",
    "        for key in port_city_mapper[pmo]:\n",
    "            if key in pier_terminal:\n",
    "                return port_city_mapper[pmo][key]\n",
    "    \n",
    "    return pier_terminal\n",
    "\n",
    "def merge_with_shapefile(shipping_df, map_df):\n",
    "    \"\"\"Merge the shipping data with the PSGC shapefile data.\"\"\"\n",
    "    mapping_file = os.path.join('..', 'ambiguos_cities.json')\n",
    "    with open(mapping_file, 'r') as f:\n",
    "        ambiguous_mapping = json.load(f)\n",
    "    \n",
    "    # Ensure all values are uppercase\n",
    "    shipping_df['city_mapping_shp'] = shipping_df['city_mapping_shp'].str.upper()\n",
    "    shipping_df['ADM2_EN'] = shipping_df['city_mapping_shp'].map(ambiguous_mapping).fillna('').str.upper()\n",
    "    map_df['ADM3_EN'] = map_df['ADM3_EN'].str.upper()\n",
    "    map_df['ADM2_EN'] = map_df['ADM2_EN'].str.upper()\n",
    "\n",
    "    # Merge normally where there is no ambiguity\n",
    "    normal_merge = shipping_df[~shipping_df['city_mapping_shp'].isin(ambiguous_mapping.keys())].merge(\n",
    "        map_df, left_on='city_mapping_shp', right_on='ADM3_EN', how='left')\n",
    "\n",
    "    # Merge with ADM2_EN for ambiguous cases\n",
    "    ambiguous_merge = shipping_df[shipping_df['city_mapping_shp'].isin(ambiguous_mapping.keys())].merge(\n",
    "        map_df, left_on=['city_mapping_shp', 'ADM2_EN'], right_on=['ADM3_EN', 'ADM2_EN'], how='left')\n",
    "    \n",
    "    # Concatenate the results\n",
    "    merged_df = pd.concat([normal_merge, ambiguous_merge], ignore_index=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def is_bold(cell):\n",
    "    \"\"\"Check if the cell text is bold.\"\"\"\n",
    "    try:\n",
    "        return cell.font.bold\n",
    "    except AttributeError:\n",
    "        return False\n",
    "\n",
    "def preprocess_shipping_data(file_path, sheet_name):\n",
    "    \"\"\"\n",
    "    Preprocesses an Excel sheet from shipping data, handling multi-level headers,\n",
    "    detecting bold cells, reformatting specific columns, and identifying the base rows\n",
    "    for each hierarchical category before forward filling, with base indicators as integers.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The file path of the Excel file.\n",
    "        sheet_name (str): The sheet name to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame with cleaned, formatted, and enhanced data.\n",
    "    \"\"\"\n",
    "    wb = openpyxl.load_workbook(file_path, data_only=True)\n",
    "    sheet = wb[sheet_name]\n",
    "\n",
    "    data, bold_info = [], []\n",
    "    for row in sheet.iter_rows(values_only=False):\n",
    "        row_data, row_bold = [], []\n",
    "        for cell in row:\n",
    "            row_data.append(cell.value)\n",
    "            row_bold.append(is_bold(cell))\n",
    "        data.append(row_data)\n",
    "        bold_info.append(row_bold)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    headers = df.iloc[:4].fillna('').astype(str).agg(' | '.join)\n",
    "    df.columns = headers\n",
    "    df = df.iloc[4:].reset_index(drop=True)\n",
    "\n",
    "    new_columns = []\n",
    "    previous_category = \"\"\n",
    "    for col in df.columns:\n",
    "        if 'Total' in col:\n",
    "            previous_category = col.split('|')[0].strip()\n",
    "        new_col_name = ' | '.join(c.strip() for c in col.split('|') if c.strip())\n",
    "        if 'Domestic' in new_col_name or 'Foreign' in new_col_name:\n",
    "            new_col_name = f\"{previous_category} | {new_col_name}\"\n",
    "        new_columns.append(new_col_name)\n",
    "\n",
    "    df.columns = new_columns\n",
    "    df.columns.values[0:3] = ['Region', 'PMO', 'Port']\n",
    "\n",
    "    is_bold_list = [1 if bold_row[2] else 0 for bold_row in bold_info[4:len(df)+4]]\n",
    "    df['is_bold'] = is_bold_list\n",
    "    df['Pier/Terminal'] = df.apply(lambda row: row['Port'] if row['is_bold'] == 0 else None, axis=1)\n",
    "    df.drop(columns=['is_bold'], inplace=True)\n",
    "    df['Port'] = df.apply(lambda row: None if row['Pier/Terminal'] == row['Port'] else row['Port'], axis=1)\n",
    "\n",
    "    df.dropna(how='all', axis=1, inplace=True)\n",
    "    df = df.loc[~(df.map(lambda x: pd.isna(x) or str(x).strip() == '')).all(axis=1)]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    column_order = ['Region', 'PMO', 'Port', 'Pier/Terminal'] + [col for col in df.columns if col not in ['Region', 'PMO', 'Port', 'Pier/Terminal']]\n",
    "    df = df[column_order]\n",
    "\n",
    "    df['Region'] = df['Region'].ffill()\n",
    "    df['PMO'] = df.groupby('Region')['PMO'].ffill()\n",
    "    df['Port'] = df.groupby(['Region', 'PMO'])['Port'].ffill()\n",
    "    df['Pier/Terminal'] = df.groupby(['Region', 'PMO', 'Port'])['Pier/Terminal'].ffill()\n",
    "\n",
    "    df['is_Region_base'] = (df['Region'].notna() & df['PMO'].isna()).astype(int)\n",
    "    df['is_PMO_base'] = (df['PMO'].notna() & df['Port'].isna()).astype(int)\n",
    "    df['is_Port_base'] = (df['Port'].notna() & df['Pier/Terminal'].isna()).astype(int)\n",
    "    df['is_Pier/Terminal_base'] = df['Pier/Terminal'].notna().astype(int)\n",
    "\n",
    "    # Create city_mapping_shp column\n",
    "    df['city_mapping_shp'] = df.apply(lambda row: map_city(row['PMO'], row['Pier/Terminal']), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "for year in tqdm(YEARS, desc=\"Processing Shipping data\"): \n",
    "    file_path = os.path.join('..', '[DATA] Ports Files', 'Raw Files', year, 'Shipping.xlsx')\n",
    "    save_path = os.path.join('..', '[DATA] Ports Files', 'Standardized', 'Shipping', f'{year} - Shipping.{SAVE_FILE_TYPE}')\n",
    "    sheet_name = 'shipcalls'\n",
    "    cleaned_data = preprocess_shipping_data(file_path, sheet_name)\n",
    "\n",
    "    # May cost time\n",
    "    # merged_data = merge_with_shapefile(cleaned_data, map_df)\n",
    "    # merged_data.to_csv(save_path, index=False)\n",
    "    cleaned_data.to_csv(save_path, index=False)\n",
    "    \n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6e0a1-a038-40b7-97ee-2d3525c0ce29",
   "metadata": {},
   "source": [
    "# Processing Passengers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf71de1-2725-447b-89c3-01b6688f9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def is_bold(cell):\n",
    "    \"\"\"Check if the cell text is bold.\"\"\"\n",
    "    try:\n",
    "        return cell.font.bold\n",
    "    except AttributeError:\n",
    "        return False\n",
    "\n",
    "def fill_and_set_bases(df, bold_info, header_rows):\n",
    "    \"\"\"Fill hierarchical categories and set base indicators, including handling for 'Pier/Terminal'.\"\"\"\n",
    "    # Ensure that the first 3 columns match the following values\n",
    "    df.columns.values[0:3] = ['Region', 'PMO', 'Port']\n",
    "\n",
    "    is_bold_list = [int(bold_row[2]) for bold_row in bold_info[header_rows:len(df)+header_rows]]\n",
    "    \n",
    "    df['is_bold'] = is_bold_list\n",
    "\n",
    "    # Use bold information to differentiate 'Port' and 'Pier/Terminal'\n",
    "    df['Pier/Terminal'] = df.apply(lambda row: row['Port'] if not row['is_bold'] else None, axis=1)\n",
    "    df['Port'] = df.apply(lambda row: row['Port'] if row['is_bold'] else None, axis=1)\n",
    "\n",
    "    # Remove the is_bold helper column as it is no longer needed. You can uncomment this for debugging purposes\n",
    "    df.drop(columns=['is_bold'], inplace=True)\n",
    "\n",
    "    df.dropna(how='all', inplace=True)  \n",
    "\n",
    "    df['Region'] = df['Region'].ffill()\n",
    "    df['PMO'] = df.groupby('Region')['PMO'].ffill()\n",
    "    df['Port'] = df.groupby(['Region', 'PMO'])['Port'].ffill()\n",
    "    df['Pier/Terminal'] = df.groupby(['Region', 'PMO', 'Port'])['Pier/Terminal'].ffill()\n",
    "\n",
    "    df['is_Region_base'] = df['Region'].notna() & df['PMO'].isna()\n",
    "    df['is_PMO_base'] = df['PMO'].notna() & df['Port'].isna()\n",
    "    df['is_Port_base'] = df['Port'].notna()\n",
    "    df['is_Pier/Terminal_base'] = df['Pier/Terminal'].notna()\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_passenger_data(file_path, sheet_name='passengers'):\n",
    "    \"\"\"\n",
    "    Preprocesses an Excel sheet from passenger data, handling multi-level headers,\n",
    "    detecting bold cells, reformatting specific columns, and identifying the base rows\n",
    "    for each hierarchical category before forward filling, with base indicators as integers.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The file path of the Excel file.\n",
    "        sheet_name (str): The sheet name to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame with cleaned, formatted, and enhanced data.\n",
    "    \"\"\"\n",
    "    wb = openpyxl.load_workbook(file_path, data_only=True)\n",
    "    sheet = wb[sheet_name]\n",
    "\n",
    "    data, bold_info = [], []\n",
    "    for row in sheet.iter_rows(values_only=False):\n",
    "        row_data, row_bold = [], []\n",
    "        for cell in row:\n",
    "            row_data.append(cell.value)\n",
    "            row_bold.append(is_bold(cell))\n",
    "        data.append(row_data)\n",
    "        bold_info.append(row_bold)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    headers = df.iloc[:4].fillna('').astype(str).agg(' | '.join)\n",
    "    df.columns = headers\n",
    "    df = df.iloc[4:].reset_index(drop=True)\n",
    "\n",
    "    new_columns = []\n",
    "    previous_category = \"\"\n",
    "    for col in df.columns:\n",
    "        if 'Total' in col:\n",
    "            previous_category = col.split('|')[0].strip()\n",
    "        new_col_name = ' | '.join(c.strip() for c in col.split('|') if c.strip())\n",
    "        if 'Disembarked' in new_col_name or 'Embarked' in new_col_name or 'Cruise Ships' in new_col_name:\n",
    "            new_col_name = f\"{previous_category} | {new_col_name}\"\n",
    "        new_columns.append(new_col_name)\n",
    "\n",
    "    df.columns = new_columns\n",
    "    df.columns.values[0:3] = ['Region', 'PMO', 'Port']\n",
    "\n",
    "    df = fill_and_set_bases(df, bold_info, 4)\n",
    "\n",
    "    # df.dropna(how='all', axis=1, inplace=True)\n",
    "    df = df.loc[~(df.map(lambda x: pd.isna(x) or str(x).strip() == '')).all(axis=1)]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    column_order = ['Region', 'PMO', 'Port', 'Pier/Terminal'] + [col for col in df.columns if col not in ['Region', 'PMO', 'Port', 'Pier/Terminal']]\n",
    "    df = df[column_order]\n",
    "\n",
    "    df['city_mapping_shp'] = df.apply(lambda row: map_city(row['PMO'], row['Pier/Terminal']), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "for year in tqdm(YEARS, desc=\"Processing Passenger data\"): \n",
    "    file_path = os.path.join('..', '[DATA] Ports Files', 'Raw Files', year, 'Passenger.xlsx')\n",
    "    save_path = os.path.join('..', '[DATA] Ports Files', 'Standardized', 'Passenger', f'{year} - Passenger.{SAVE_FILE_TYPE}')\n",
    "    sheet_name = 'passengers'\n",
    "    cleaned_data = preprocess_passenger_data(file_path, sheet_name)\n",
    "\n",
    "    cleaned_data.to_csv(save_path, index=False)\n",
    "    \n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce647d-2ad1-410f-b805-e7cd59f11a70",
   "metadata": {},
   "source": [
    "# Processing Cargo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabeff59-0bd0-4aa2-8a2a-ba06927ba913",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def is_bold(cell):\n",
    "    \"\"\"Check if the cell text is bold.\"\"\"\n",
    "    try:\n",
    "        return cell.font.bold\n",
    "    except AttributeError:\n",
    "        return False\n",
    "\n",
    "def preprocess_cargo_data(file_path, sheet_name='cargo', header_rows=4):\n",
    "    \"\"\"Preprocesses an Excel sheet from cargo data.\"\"\"\n",
    "    wb = openpyxl.load_workbook(file_path, data_only=True)\n",
    "    sheet = wb[sheet_name]\n",
    "\n",
    "    data, bold_info = [], []\n",
    "    for row in sheet.iter_rows(values_only=False):\n",
    "        row_data, row_bold = [cell.value for cell in row], [is_bold(cell) for cell in row]\n",
    "        data.append(row_data)\n",
    "        bold_info.append(row_bold)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.dropna(how='all', axis=1, inplace=True)  \n",
    "    \n",
    "    headers = df.iloc[:header_rows].fillna('').astype(str).agg(' | '.join, axis=0)\n",
    "    df.columns = headers\n",
    "    df = df.iloc[header_rows:].reset_index(drop=True)\n",
    "\n",
    "    df.columns = enhance_column_names(df.columns)\n",
    "      \n",
    "\n",
    "    df = fill_and_set_bases(df, bold_info, header_rows)\n",
    "\n",
    "    column_order = ['Region', 'PMO', 'Port', 'Pier/Terminal'] + [col for col in df.columns if col not in ['Region', 'PMO', 'Port', 'Pier/Terminal']]\n",
    "    df = df[column_order]\n",
    "\n",
    "    df['city_mapping_shp'] = df.apply(lambda row: map_city(row['PMO'], row['Pier/Terminal']), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def enhance_column_names(columns):\n",
    "    \"\"\"Enhances column names to include hierarchical information correctly.\"\"\"\n",
    "    new_columns = []\n",
    "    current_month = \"\"\n",
    "    current_category = \"\"\n",
    "    for col in columns:\n",
    "        parts = col.split(' | ')\n",
    "        if 'GRAND TOTAL' in col:\n",
    "            month = parts[0]\n",
    "            if any(m in month for m in ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']):\n",
    "                current_month = month\n",
    "                current_category = \"\"\n",
    "        elif 'DOMESTIC' in col or 'FOREIGN' in col:\n",
    "            current_category = parts[0]\n",
    "        new_header = f\"{current_month} | {current_category} | {' | '.join(parts[1:])}\"\n",
    "        new_columns.append(new_header)\n",
    "    return new_columns\n",
    "\n",
    "def fill_and_set_bases(df, bold_info, header_rows):\n",
    "    \"\"\"Fill hierarchical categories and set base indicators, including handling for 'Pier/Terminal'.\"\"\"\n",
    "    # Pls dont touch these (Although, ensure that first 3 cols match the following vals)\n",
    "    df.columns.values[0:3] = ['Region', 'PMO', 'Port']\n",
    "\n",
    "    is_bold_list = [int(bold_row[2]) for bold_row in bold_info[header_rows:len(df)+header_rows]]\n",
    "\n",
    "    df['is_bold'] = is_bold_list\n",
    "\n",
    "    # Use bold information to differentiate 'Port' and 'Pier/Terminal'\n",
    "    df['Pier/Terminal'] = df.apply(lambda row: row['Port'] if not row['is_bold'] else None, axis=1)\n",
    "    df['Port'] = df.apply(lambda row: row['Port'] if row['is_bold'] else None, axis=1)\n",
    "\n",
    "    # Remove the is_bold helper column as it is no longer needed. You can uncomment ths for dbeugging purposes\n",
    "    df.drop(columns=['is_bold'], inplace=True)\n",
    "\n",
    "    df.dropna(how='all', inplace=True)  \n",
    "\n",
    "    df['Region'] = df['Region'].ffill()\n",
    "    df['PMO'] = df.groupby('Region')['PMO'].ffill()\n",
    "    df['Port'] = df.groupby(['Region', 'PMO'])['Port'].ffill()\n",
    "    df['Pier/Terminal'] = df.groupby(['Region', 'PMO', 'Port'])['Pier/Terminal'].ffill()\n",
    "\n",
    "    df['is_Region_base'] = df['Region'].notna() & df['PMO'].isna()\n",
    "    df['is_PMO_base'] = df['PMO'].notna() & df['Port'].isna()\n",
    "    df['is_Port_base'] = df['Port'].notna()\n",
    "    df['is_Pier/Terminal_base'] = df['Pier/Terminal'].notna()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "for year in tqdm(YEARS, desc=\"Processing Cargo data\"):\n",
    "    file_path = os.path.join('..', '[DATA] Ports Files', 'Raw Files', year, 'Cargo.xlsx')\n",
    "    save_path = os.path.join('..', '[DATA] Ports Files', 'Standardized', 'Cargo', f'{year} - Cargo.{SAVE_FILE_TYPE}')\n",
    "    sheet_name = 'cargo'\n",
    "    cleaned_data = preprocess_cargo_data(file_path, sheet_name, 4)\n",
    "    \n",
    "# Put mappings here\n",
    "    \n",
    "    cleaned_data.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f065db-d495-4a83-815d-111055fb8f2a",
   "metadata": {},
   "source": [
    "# Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c52dbd-bc07-4ca8-acfe-38bb45f0fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "def is_bold(cell):\n",
    "    \"\"\"Check if the cell text is bold.\"\"\"\n",
    "    try:\n",
    "        return cell.font.bold\n",
    "    except AttributeError:\n",
    "        return False\n",
    "\n",
    "def preprocess_container_data(file_path, sheet_name='container', header_rows=4):\n",
    "    \"\"\"Preprocesses an Excel sheet from container data.\"\"\"\n",
    "    wb = openpyxl.load_workbook(file_path, data_only=True)\n",
    "    sheet = wb[sheet_name]\n",
    "\n",
    "    data, bold_info = [], []\n",
    "    for row in sheet.iter_rows(values_only=False):\n",
    "        row_data, row_bold = [cell.value for cell in row], [is_bold(cell) for cell in row]\n",
    "        data.append(row_data)\n",
    "        bold_info.append(row_bold)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.dropna(how='all', axis=1, inplace=True)  \n",
    "    \n",
    "    headers = df.iloc[:header_rows].fillna('').astype(str).agg(' | '.join, axis=0)\n",
    "    df.columns = headers\n",
    "    df = df.iloc[header_rows:].reset_index(drop=True)\n",
    "\n",
    "    df.columns = enhance_column_names(df.columns)\n",
    "    # df.dropna(how='all', inplace=True)  \n",
    "\n",
    "    df = fill_and_set_bases(df, bold_info, header_rows)\n",
    "\n",
    "    column_order = ['Region', 'PMO', 'Port', 'Pier/Terminal'] + [col for col in df.columns if col not in ['Region', 'PMO', 'Port', 'Pier/Terminal']]\n",
    "    df = df[column_order]\n",
    "\n",
    "    df['city_mapping_shp'] = df.apply(lambda row: map_city(row['PMO'], row['Pier/Terminal']), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def enhance_column_names(columns):\n",
    "    \"\"\"Enhances column names to include hierarchical information correctly.\"\"\"\n",
    "    new_columns = []\n",
    "    current_month = \"\"\n",
    "    current_category = \"\"\n",
    "    for col in columns:\n",
    "        parts = col.split(' | ')\n",
    "        if 'GRAND TOTAL' in col:\n",
    "            month = parts[0]\n",
    "            if any(m in month for m in ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']):\n",
    "                current_month = month\n",
    "                current_category = \"\"\n",
    "        elif 'DOMESTIC' in col or 'FOREIGN' in col:\n",
    "            current_category = parts[0]\n",
    "        new_header = f\"{current_month} | {current_category} | {' | '.join(parts[1:])}\"\n",
    "        new_columns.append(new_header)\n",
    "    return new_columns\n",
    "\n",
    "def fill_and_set_bases(df, bold_info, header_rows):\n",
    "    \"\"\"Fill hierarchical categories and set base indicators, including handling for 'Pier/Terminal'.\"\"\"\n",
    "    # Pls dont touch these (Although, ensure that first 3 cols match the following vals)\n",
    "    df.columns.values[0:3] = ['Region', 'PMO', 'Port']\n",
    "\n",
    "    is_bold_list = [int(bold_row[2]) for bold_row in bold_info[header_rows:len(df)+header_rows]]\n",
    "\n",
    "    # Shift is_bold list up by one row\n",
    "    # if len(is_bold_list) > 1:\n",
    "    #     is_bold_list = is_bold_list[1:] + [0]\n",
    "    \n",
    "    df['is_bold'] = is_bold_list\n",
    "\n",
    "    # Use bold information to differentiate 'Port' and 'Pier/Terminal'\n",
    "    df['Pier/Terminal'] = df.apply(lambda row: row['Port'] if not row['is_bold'] else None, axis=1)\n",
    "    df['Port'] = df.apply(lambda row: row['Port'] if row['is_bold'] else None, axis=1)\n",
    "\n",
    "    # Remove the is_bold helper column as it is no longer needed. You can uncomment ths for dbeugging purposes\n",
    "    df.drop(columns=['is_bold'], inplace=True)\n",
    "\n",
    "    df.dropna(how='all', inplace=True)  \n",
    "\n",
    "    df['Region'] = df['Region'].ffill()\n",
    "    df['PMO'] = df.groupby('Region')['PMO'].ffill()\n",
    "    df['Port'] = df.groupby(['Region', 'PMO'])['Port'].ffill()\n",
    "    df['Pier/Terminal'] = df.groupby(['Region', 'PMO', 'Port'])['Pier/Terminal'].ffill()\n",
    "\n",
    "    df['is_Region_base'] = df['Region'].notna() & df['PMO'].isna()\n",
    "    df['is_PMO_base'] = df['PMO'].notna() & df['Port'].isna()\n",
    "    df['is_Port_base'] = df['Port'].notna()\n",
    "    df['is_Pier/Terminal_base'] = df['Pier/Terminal'].notna()\n",
    "\n",
    "    return df\n",
    "\n",
    "file_name = 'Container'\n",
    "\n",
    "for year in tqdm(YEARS, desc=f\"Processing {file_name} data\"):\n",
    "    file_path = os.path.join('..', '[DATA] Ports Files', 'Raw Files', year, f'{file_name}.xlsx')\n",
    "    save_path = os.path.join('..', '[DATA] Ports Files', 'Standardized', file_name, f'{year} - {file_name}.{SAVE_FILE_TYPE}')\n",
    "    sheet_name = 'teu'\n",
    "    cleaned_data = preprocess_container_data(file_path, sheet_name, 4)  # Pass header_rows explicitly\n",
    "    cleaned_data.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db707a3d-d273-460a-8eff-72631089f3b1",
   "metadata": {},
   "source": [
    "# RORO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714930e-bb2d-4543-9092-d1b47c38606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def is_bold(cell):\n",
    "    \"\"\"Check if the cell text is bold.\"\"\"\n",
    "    try:\n",
    "        return cell.font.bold\n",
    "    except AttributeError:\n",
    "        return False\n",
    "\n",
    "def preprocess_roro_data(file_path, sheet_name='roro', header_rows=4):\n",
    "    \"\"\"Preprocesses an Excel sheet from roro data.\"\"\"\n",
    "    wb = openpyxl.load_workbook(file_path, data_only=True)\n",
    "    sheet = wb[sheet_name]\n",
    "\n",
    "    data, bold_info = [], []\n",
    "    for row in sheet.iter_rows(values_only=False):\n",
    "        row_data, row_bold = [cell.value for cell in row], [is_bold(cell) for cell in row]\n",
    "        data.append(row_data)\n",
    "        bold_info.append(row_bold)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.dropna(how='all', axis=1, inplace=True)  \n",
    "    \n",
    "    headers = df.iloc[:header_rows].fillna('').astype(str).agg(' | '.join, axis=0)\n",
    "    df.columns = headers\n",
    "    df = df.iloc[header_rows:].reset_index(drop=True)\n",
    "\n",
    "    df.columns = enhance_column_names(df.columns)\n",
    "    \n",
    "\n",
    "    df = fill_and_set_bases(df, bold_info, header_rows)\n",
    "    \n",
    "\n",
    "    column_order = ['Region', 'PMO', 'Port', 'Pier/Terminal'] + [col for col in df.columns if col not in ['Region', 'PMO', 'Port', 'Pier/Terminal']]\n",
    "    df = df[column_order]\n",
    "\n",
    "    df['city_mapping_shp'] = df.apply(lambda row: map_city(row['PMO'], row['Pier/Terminal']), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def enhance_column_names(columns):\n",
    "    \"\"\"Enhances column names to include hierarchical information, correctly handling nested categories.\"\"\"\n",
    "    new_columns = []\n",
    "    current_month = \"\"\n",
    "    current_direction = \"\"\n",
    "    current_type = \"\"       \n",
    "\n",
    "    for col in columns:\n",
    "        parts = [part.strip() for part in col.split(' | ') if part.strip()]\n",
    "        if parts:\n",
    "            if any(month in parts[0] for month in ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']):\n",
    "                current_month = parts[0]  \n",
    "                current_direction = \"\" \n",
    "                current_type = \"\"  \n",
    "            if 'Inbound' in parts or 'Outbound' in parts:\n",
    "                current_direction = parts[0] \n",
    "                current_type = \"\"  \n",
    "            if 'Type' in parts[-1]:\n",
    "                current_type = parts[-1]  \n",
    "\n",
    "            if current_type:\n",
    "                new_header = f\"{current_month} | {current_direction} | {current_type}\"\n",
    "            else:\n",
    "                new_header = f\"{current_month} | {current_direction} | {' | '.join(parts[1:])}\"\n",
    "            new_columns.append(new_header)\n",
    "        else:\n",
    "            new_columns.append(f\"{current_month} | {current_direction} | {current_type}\")\n",
    "\n",
    "    return new_columns\n",
    "\n",
    "def fill_and_set_bases(df, bold_info, header_rows):\n",
    "    \"\"\"Fill hierarchical categories and set base indicators, including handling for 'Pier/Terminal'.\"\"\"\n",
    "    # Pls dont touch these (Although, ensure that first 3 cols match the following vals)\n",
    "    df.columns.values[0:3] = ['Region', 'PMO', 'Port']\n",
    "\n",
    "    is_bold_list = [int(bold_row[2]) for bold_row in bold_info[header_rows:len(df)+header_rows]]\n",
    "    \n",
    "    df['is_bold'] = is_bold_list\n",
    "\n",
    "    # Use bold information to differentiate 'Port' and 'Pier/Terminal'\n",
    "    df['Pier/Terminal'] = df.apply(lambda row: row['Port'] if not row['is_bold'] else None, axis=1)\n",
    "    df['Port'] = df.apply(lambda row: row['Port'] if row['is_bold'] else None, axis=1)\n",
    "\n",
    "    # Remove the is_bold helper column as it is no longer needed. You can uncomment ths for dbeugging purposes\n",
    "    df.drop(columns=['is_bold'], inplace=True)\n",
    "\n",
    "    df.dropna(how='all', inplace=True)  \n",
    "\n",
    "    df['Region'] = df['Region'].ffill()\n",
    "    df['PMO'] = df.groupby('Region')['PMO'].ffill()\n",
    "    df['Port'] = df.groupby(['Region', 'PMO'])['Port'].ffill()\n",
    "    df['Pier/Terminal'] = df.groupby(['Region', 'PMO', 'Port'])['Pier/Terminal'].ffill()\n",
    "\n",
    "    df['is_Region_base'] = df['Region'].notna() & df['PMO'].isna()\n",
    "    df['is_PMO_base'] = df['PMO'].notna() & df['Port'].isna()\n",
    "    df['is_Port_base'] = df['Port'].notna()\n",
    "    df['is_Pier/Terminal_base'] = df['Pier/Terminal'].notna()\n",
    "\n",
    "    return df\n",
    "\n",
    "file_name = 'RORO'\n",
    "\n",
    "for year in tqdm(YEARS[1:], desc=f\"Processing {file_name} data\"):\n",
    "    file_path = os.path.join('..', '[DATA] Ports Files', 'Raw Files', year, f'{file_name}.xlsx')\n",
    "    save_path = os.path.join('..', '[DATA] Ports Files', 'Standardized', file_name, f'{year} - {file_name}.{SAVE_FILE_TYPE}')\n",
    "    sheet_name = 'roro'\n",
    "    cleaned_data = preprocess_roro_data(file_path, sheet_name, 4) \n",
    "    cleaned_data.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20db9d1-d342-4dba-8df6-696c749e52fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
